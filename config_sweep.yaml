# Example sweep configuration for hyperparameter optimization with wandb
# Copy and edit this file as needed for your experiments and environment.

program: wbsweep.py

# --- Sweep/Hyperparameter Search Settings ---
# method: grid  # Options: grid, random, bayes
method: grid

# --- Metric to Optimize ---
metric:
  name: Validation/f1_score
  goal: maximize

# --- wandb Project Info ---
project: underxai
# entity: your_wandb_entity  # Uncomment and set if needed

# --- Hyperparameter Search Space ---
parameters:
  accumulation_steps:
    value: 16

  attention_resc:
    value: 2

  batch_size:
    value: 8

  clinical_features:
    value: False
  
  clinical_input_dim:
    value: 4
  
  clinical_encoding_dim:
    value: 4

  dropout:
    value: 0.25
    
  early_stopping_patience:
    value: 70

  epochs:
    value: 100

  feature_extractor:
    values: ["dinov2-small", "google-32"]
  
  learning_rate:
    value: 1.0e-4

  loss_function:
    value: 'WBCE'  

  num_classes:
    value: 1

  num_heads:
    value: 4

  optimizer:
    value: 'adamW' 

  scheduler_gamma:
    value: 0.5

  scheduler_patience:
    value: 60

  scheduler_step:
    value: 5

  scheduler_type:
    value: 'step'
  
  weight_decay:
    value: 1.0e-6


  # --- Data Paths ---
  # Set these to the correct locations for your data files
  train_paths:
    value: 'PATH/TO/train_dataset_ids.json'  # e.g., './data/train_dataset_ids.json'
  val_paths:
    value: 'PATH/TO/val_dataset_ids.json'    # e.g., './data/val_dataset_ids.json'
  test_paths:
    value: 'PATH/TO/test_dataset_ids.json'   # e.g., './data/test_dataset_ids.json'









  








    


